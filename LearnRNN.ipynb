{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn simple layer of RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add conv layer\n",
    "add l2 regularizer\n",
    "add LSTMBlockFused\n",
    "remove dropout to add optimized graph, add quantized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1.10.1 of tensorflow\n",
      "maxSize = 6726407\n",
      "shape input train (6726258, 150)\n",
      "Data loaded\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/totovai/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -0 calculated in 58.09 s \n",
      "Epoch 0 MSE 0.20629, RMSENorm: 0.45976 on test set with deviation of 20.42%\n",
      "Epoch -1 calculated in 57.07 s \n",
      "Epoch 1 MSE 0.19245, RMSENorm: 0.42893 on test set with deviation of 34.85%\n",
      "Epoch -2 calculated in 56.15 s \n",
      "Epoch 2 MSE 0.18301, RMSENorm: 0.40788 on test set with deviation of 46.43%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4133ec83df1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mRMSETrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpMSETrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mno_of_batchesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('Codes')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dataShaping import *\n",
    "from savePerf import *\n",
    "from saveTransformedGraph import optimizeGraph\n",
    "import scipy.io.wavfile\n",
    "import time\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelName = \"RNN1\"\n",
    "# create directory experiment\n",
    "date = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "path = os.path.join(\"Experiments\",date)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    #experiment/\"date\"/temp will contain the backuped model parameters\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "    # if you run the file two time in a minute\n",
    "else :\n",
    "    date = date+'(2)'\n",
    "    path = os.path.join(\"Experiments\",date)\n",
    "    os.makedirs(path)\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "\n",
    "# directory that will contain tensorboard information\n",
    "pathLog = 'Tf_logs'\n",
    "if not os.path.isdir(pathLog):\n",
    "    os.makedirs(pathLog)\n",
    "pathLog = \"{}/run-{}/\".format(pathLog,date)\n",
    "\n",
    "version = tf.__version__\n",
    "print (\"version {} of tensorflow\".format(version))\n",
    "\n",
    "#############################\n",
    "# Model parameters\n",
    "#############################\n",
    "trainTestRatio = 0.8\n",
    "#if you cannot load all the data set in Ram specify wich part you want to load (0 means all the dataset)\n",
    "maxSize = 0\n",
    "num_step = 150                                       # time step before reduction\n",
    "conv_chan = 35                                         #number of kernel for convolution\n",
    "conv_strides = 3#int(np.ceil(num_step/num_LSTM))                  #decay between two convolution\n",
    "conv_size = 12                                         #filter size for the convolution\n",
    "size_poll = 4\n",
    "reg_scale = 0\n",
    "l1l2Prop = 0.4  # 1 =>l1, 0=> l2\n",
    "reg_scale_l1 = l1l2Prop*reg_scale\n",
    "reg_scale_l2 = ((1-l1l2Prop)/2)*reg_scale\n",
    "num_hidden = 200                                      #num of hidden units\n",
    "num_class = 1                                          #size of the output\n",
    "num_feature = 1                                        # size of the input\n",
    "batch_size = 2500                                  # number of sequence taken before to compute the gradient\n",
    "n_layer = 1                                             #num_layer\n",
    "\n",
    "#num_hidden = num_hidden/keep_prob\n",
    "num_epoch = 100000                                      # process all the datas num_epoch times\n",
    "trainDuration = 60*60*15                             # or during a determined duration(second)\n",
    "amplifierName = 'MesaMarkVCrunch'\n",
    "fileNameTrain = 'Datasets/training'+amplifierName+'.mat'             #dataset train/test path\n",
    "fileNameTest = 'Datasets/test'+amplifierName+'.mat' # dataset validation path\n",
    "fileNameValidation = 'Datasets/val'+amplifierName+'.mat'\n",
    "\n",
    "#############################\n",
    "# Loading data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileNameTrain)\n",
    "matrixTrain = matrix['train']\n",
    "matrix = sio.loadmat(fileNameTest)\n",
    "matrixTest = matrix['test']\n",
    "if maxSize ==0:\n",
    "    maxSize = len(matrixTrain)\n",
    "    print(\"maxSize = {}\".format(maxSize))\n",
    "\n",
    "train_input,train_output,test_input,test_output = loadInputOutputSeq(matrixTrain,matrixTest,num_step,maxSize)\n",
    "\n",
    "\n",
    "print(\"shape input train {}\".format(np.shape(train_input)))\n",
    "numTrain = len(train_output)\n",
    "print (\"Data loaded\")\n",
    "#######################\n",
    "#Graph\n",
    "#######################\n",
    "\n",
    "G = tf.Graph()\n",
    "with G.as_default():\n",
    "    with tf.name_scope(\"placeHolder\"):\n",
    "        data = tf.placeholder(tf.float32, [None, num_step], name =\"data\") #Number of examples, number of input step (time step), dimension of each input\n",
    "        target = tf.placeholder(tf.float32, [None, num_class],name = \"target\") # batchSize, nbClass\n",
    "\n",
    "    dataShaped = tf.reshape(data,[tf.shape(data)[0],num_step,1,1]) # batchSize, num_step width channel\n",
    "    with tf.variable_scope(\"ConvLayers\"):\n",
    "        regularizerC1 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC1\")\n",
    "        dataReduced = tf.layers.conv2d(inputs = dataShaped,filters = conv_chan,\n",
    "                                       kernel_size = (conv_size,1),strides=(4,1),\n",
    "                                       padding = \"same\",activation=tf.nn.elu,kernel_regularizer=regularizerC1,name=\"C1\")#batch_size num_Lstm num_channel\n",
    "        regularizerC2 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC2\")\n",
    "        dataReduced = tf.layers.conv2d(inputs = dataReduced,filters = conv_chan,\n",
    "                                       kernel_size = (conv_size,1),strides=(3,1),\n",
    "                                       padding = \"same\",activation=tf.nn.elu,kernel_regularizer=regularizerC2,name=\"C2\")#batch_size num_Lstm num_channel\n",
    "        \n",
    "    dataReduced = tf.reshape(dataReduced,[tf.shape(data)[0],tf.shape(dataReduced)[1],conv_chan]) #NHC\n",
    "    \n",
    "    fusedCell = tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
    "    fusedCellAdaptator = tf.contrib.rnn.FusedRNNCellAdaptor(fusedCell,use_dynamic_rnn=True)\n",
    "\n",
    "    dataReduced = tf.transpose(dataReduced,[1,0,2]) #HNC\n",
    "\n",
    "    with tf.name_scope(\"extractLastValueLSTM\"):\n",
    "        val,states = fusedCellAdaptator(dataReduced,initial_state=tf.zeros([tf.shape(data)[0],num_hidden], dtype=tf.float32)) # val dim is [num_step,batch_size, numhidden]\n",
    "        \n",
    "    # Let's first fetch the last index of seq length\n",
    "    # last_index would have a scalar value\n",
    "        last_index = tf.shape(val)[0] - 1\n",
    "        last = tf.gather(val,last_index)\n",
    "\n",
    "   \n",
    "    prediction = fully_connected(last,int(target.get_shape()[1]),activation_fn=tf.nn.tanh,weights_regularizer=None,scope=\"FCPred\")\n",
    "    #Compute the mean square error\n",
    "    MSE = tf.reduce_mean(tf.square(prediction-target))\n",
    "    EnergyTarget = tf.reduce_mean(tf.square(target)) \n",
    "    \n",
    "    #get regularizer\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    MSEReg = tf.add_n([MSE]+reg_losses,name=\"MSEReg\")\n",
    "    # create optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    #Compute gradient and apply backpropagation\n",
    "    minimize = optimizer.minimize(MSEReg)\n",
    "\n",
    "    # Create summary view for tensorboard\n",
    "    mse_summary = tf.summary.scalar('RMSE',tf.sqrt(MSE))\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    #Create an init op to initialize variable\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # save variable, use saver.restore(sess,\"date/tmp/my_model.ckpt\") instead of sess.run(init_op)\n",
    "\n",
    "##############################\n",
    "# Execution du graphe\n",
    "##############################\n",
    "    \n",
    "with tf.Session(graph=G) as sess:\n",
    "    #restorePath = os.path.join('2017-09-11-18-07','temp','my_model.ckpt') # example for restore a previous model\n",
    "    #saver.restore(sess,restorePath)\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(pathLog+'train',graph =tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(pathLog+'test')\n",
    "\n",
    "    no_of_batches = int(np.floor((numTrain)/batch_size)) # numtrain -numstep ?\n",
    "    no_of_batchesTest = int(np.floor((len(test_input))/batch_size))\n",
    "    tStart = time.clock()\n",
    "    epoch =0\n",
    "    for epoch in range(num_epoch):\n",
    "        tEpoch = time.clock()\n",
    "        if (time.clock()-tStart < trainDuration):\n",
    "            ptr = 0\n",
    "            if epoch % 20==0 : # each ten epoch save the model\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myGraph.pb',as_text=False)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myModel.ckpt'))\n",
    "            pMSETrain=0\n",
    "            for j in range(no_of_batches):\n",
    "                inp, out = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size]\n",
    "                ptr+=batch_size\n",
    "                \n",
    "                if j % np.floor(numTrain/len(test_input)) ==0 : # This is to have a train summary and a test summary of the same size\n",
    "                    _,summary_str,pMSETrainTemp,pLast = sess.run([minimize,summary_op,MSE,last],{data: inp, target: out})\n",
    "\n",
    "                    pMSETrain += pMSETrainTemp\n",
    "                    step = epoch*no_of_batches+j\n",
    "                    train_writer.add_summary(summary_str,step)\n",
    "                else :\n",
    "                    sess.run([minimize],{data: inp, target: out})\n",
    "                   \n",
    "            RMSETrain = np.sqrt(pMSETrain/no_of_batchesTest)\n",
    "            print (\"Epoch -{} calculated in {:5.2f} s \".format(epoch,time.clock()-tEpoch))\n",
    "            # evaluate the model on the test set (compute the mean of the MSE)\n",
    "            pMSE = 0\n",
    "            ptr2 = 0\n",
    "            pEnergyTarget = 0\n",
    "            for k in range(no_of_batchesTest):\n",
    "                pMSETemp,pEnergyTargetTemp,summary_str = sess.run([MSE,EnergyTarget,summary_op],{data: test_input[ptr2:ptr2+batch_size] , target: test_output[ptr2:ptr2+batch_size]})\n",
    "                pMSE += pMSETemp\n",
    "                ptr2 += batch_size\n",
    "                pEnergyTarget+=pEnergyTargetTemp\n",
    "                step = epoch*no_of_batchesTest+k\n",
    "                test_writer.add_summary(summary_str,step*np.floor(numTrain/len(test_input)))\n",
    "            RMSETest = np.sqrt(pMSE/no_of_batchesTest)\n",
    "            RMSENorm = np.sqrt(pMSE/pEnergyTarget)\n",
    "            print(\"Epoch {} MSE {:.5}, RMSENorm: {:.5} on test set with deviation of {:.2f}%\".format(epoch,RMSETest,RMSENorm,100*np.sqrt((RMSETrain-RMSETest)**2)/RMSETrain))\n",
    "        else : break # break the while loop if number of epoch is reached\n",
    "    tStop = time.clock()\n",
    "    trainTime = time.strftime(\"%d:%H:%M:%S \", time.gmtime(tStop-tStart))\n",
    "    \n",
    "    #######################\n",
    "    # Save Graph variable and information about the running session\n",
    "    #######################\n",
    "    # save graph model\n",
    "    tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myFinalGraph.pbtxt',as_text=True)\n",
    "    # Save checkpoint variables\n",
    "    save_path = saver.save(sess,os.path.join(pathTemp,'myFinalModel.ckpt'))\n",
    "    print (\"Training duration {}\".format(trainTime))\n",
    "    totalParameters =np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()])\n",
    "    print(\"Number of training variable {}\".format(totalParameters))\n",
    "    # log\n",
    "    infoLog={}\n",
    "    infoLog[\"path\"] = path\n",
    "    infoLog[\"MSE\"] = np.sqrt(pMSE/no_of_batchesTest)\n",
    "    infoLog[\"num_step\"] = num_step\n",
    "    infoLog[\"num_hidden\"] = num_hidden\n",
    "    infoLog[\"num_epoch\"] = epoch\n",
    "    infoLog[\"batch_size\"] = batch_size\n",
    "    infoLog[\"maxSize\"] = maxSize\n",
    "    infoLog[\"duration\"] = trainTime\n",
    "    infoLog[\"totalParameters\"] = totalParameters\n",
    "    infoLog[\"version\"] = version\n",
    "    infoLog[\"n_layer\"] = n_layer\n",
    "    infoLog[\"trainDropout\"] = 0\n",
    "    infoLog[\"nameModel\"] = modelName\n",
    "    infoLog[\"conv_chan\"] = conv_chan\n",
    "    infoLog[\"strides\"] = conv_strides\n",
    "    infoLog[\"conv_size\"] = conv_size\n",
    "    infoLog[\"amplifierName\"]=amplifierName\n",
    "    logPerf(infoLog)\n",
    "    input_nodes=[\"placeHolder/data\"]\n",
    "    output_nodes=[\"FCPred/Tanh\"]\n",
    "    optimizeGraph(pathTemp,input_nodes,output_nodes)\n",
    "    \n",
    "    \n",
    "                                                 \n",
    "    \n",
    "    ###############################\n",
    "    #   validation dataset and emulate guitar signal\n",
    "    ###############################\n",
    "    matrixVal = sio.loadmat(fileNameValidation)\n",
    "    matrixVal = matrixVal['val']  \n",
    "    valSize = 0\n",
    "    if valSize == 0 :\n",
    "        valSize = len(matrixVal)\n",
    "    # shape validation test\n",
    "    val_input,val_output = loadValidationSeq(matrixVal,num_step,valSize)\n",
    "    lPrediction = []\n",
    "    lTarget = []\n",
    "    ptr3 = 0\n",
    "    no_of_batchesVal = int(np.floor((len(val_input))/batch_size))\n",
    "    for k in range(no_of_batchesVal):\n",
    "        pPrediction,pTarget = sess.run([prediction,target],{data: val_input[ptr3:ptr3+batch_size], target: val_output[ptr3:ptr3+batch_size]}) \n",
    "        lPrediction.append(pPrediction)\n",
    "        lTarget.append(pTarget)   \n",
    "        ptr3+=batch_size\n",
    "    #plt.show()scree\n",
    "    predictionArray = np.array(lPrediction,dtype=np.float32).ravel()\n",
    "    targetArray = np.array(lTarget,dtype=np.float32).ravel()\n",
    "    scipy.io.wavfile.write(os.path.join(path,'prediction.wav'),44100,predictionArray)\n",
    "    scipy.io.wavfile.write(os.path.join(path,'target.wav'),44100,targetArray)\n",
    "\n",
    "    # save emulation in a pickle format\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(predictionArray[:10000],label='prediction')\n",
    "    ax.plot(targetArray[:10000],label='target')\n",
    "    ax.legend()\n",
    "    nameFigEstimation = os.path.join(path,\"targetVsPrediction.pickle\")\n",
    "    pickle.dump(ax,open(nameFigEstimation, 'wb'))\n",
    "print (\"done, good job kids\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from show import *\n",
    "%matplotlib notebook\n",
    "showPickle(nameFigEstimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
